---
title: Insert title here
key: 37d7d11af5613069547b123ccc47ed77

---
## Handling Errors and Missing Data

```yaml
type: "TitleSlide"
key: "e8067e90dc"
```

`@lower_third`

name: Amany Mahfouz
title: undefined


`@script`
So far, you’ve worked on importing different flat files, like csvs and tsvs, and customizing imports to set better field names and get only the data you're interested in. All of this is great if your data is in really great shape to begin with. But what if there are issues with the data or the import?


---
## Common Flat File Import Issues

```yaml
type: "FullSlide"
key: "4e9d475a9d"
```

`@part1`
* Column data types are wrong
* Values are missing
* Lines contain errors or are blank


`@script`
Datasets often have missing values, missing values encoded in unexpected ways, or records that have errors or are entirely blank; sometimes, data is read in as the wrong type, making it harder to analyze. Luckily, the pandas read_csv method offers ways to address these issues right at the import stage, reducing the wrangling you have to do later on.


---
## Specifying Data Types

```yaml
type: "TwoRows"
key: "22caa3e64d"
```

`@part1`
* Pandas will try to guess column data types, but it doesn't always get it right
* Use the `dtype` parameter in `read_csv` to specify any or all column data types
* `dtype` takes a dictionary as an argument


`@part2`
```
import pandas as pd

data = pd.read_csv(file, dtype={‘zipcode’: str})
```


`@script`
When you read a csv with pandas, pandas infers each column's data type. However, it doesn’t always get it right. For example, here is a csv of 2016 tax return data for the state of Vermont by ZIP code. Pandas interprets zipcode as an integer field, but it should be strings.
Instead of letting pandas guess, we can tell it exactly what data type any or all of the fields in a csv should be using the ```dtype``` parameter in ```read_csv```.
```dtype``` takes a dictionary, where each key is a field and each value is the data type that field should be. Here, we ensure ZIP codes are strings with the following code:


---
## Setting Custom Missing Data Values

```yaml
type: "TwoRows"
key: "8e8f83d823"
```

`@part1`



`@part2`
```
import pandas as pd

data = pd.read_csv(file, na_values=[])
```


`@script`
Pandas automatically looks out for certain values, like “N/A” or “null”,  when importing data and re-codes them as missing values, allowing you to make use of some handy data-cleaning functions later on.  But sometimes, missing values in your data might be represented differently, such as with a dummy code or the text “none”.
You can tell pandas to treat these entries as missing values with the ```na_values``` parameter in ```read_csv```.
```na_values``` can take a list of values that should be coded as N/As or NaNs. You can even set column-specific N/A values by passing in a dictionary, where each key-value pair represents a field and a list of the values in that field to treat as N/As.


---
## Blank Lines and Lines with Errors

```yaml
type: "TwoRows"
key: "e03fcbdf7b"
```

`@part1`



`@part2`
```
import pandas as pd
data = pd.read_csv(file, error_bad_lines=False, warn_bad_lines=True)
```


`@script`
One last data issue you may face are lines of entirely bad data. For example, a record could have too many fields, in which case, pandas won't import the csv at all. You can tell pandas to skip over these lines using a few parameters in read_csv. Setting error_bad_lines to False will cause pandas to skip bad records but continue the import. Pandas won't tell you it's skipping lines unless you set warn_bad_lines to True as well, though.

A word of caution: if you find that lines are being skipped due to errors, it's a good idea to investigate what's being left out and why. You might find an underlying data issue that can be addressed, or the issue could introduce bias into your analysis that will have to be accounted for.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "bc2d10733a"
```

`@script`
In this video, you've learned about some common issues analysts face when importing data from csvs, and how to handle those issues. Now, it's time to practice. Good luck!


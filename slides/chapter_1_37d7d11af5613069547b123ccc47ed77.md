---
title: Insert title here
key: 37d7d11af5613069547b123ccc47ed77

---
## Handling Errors and Missing Data

```yaml
type: "TitleSlide"
key: "e8067e90dc"
```

`@lower_third`

name: Amany Mahfouz
title: 


`@script`
So far, you’ve worked on importing different flat files, like csvs and tsvs, and customizing imports to set better field names and get only the data you're interested in. That might be enough if your data is in great shape to begin with. But what if there are issues with the data or the import?


---
## Common Flat File Import Issues

```yaml
type: "FullSlide"
key: "4e9d475a9d"
```

`@part1`
* Column data types are wrong {{1}}
* Values are missing {{2}}
* Lines contain errors, like too many fields {{3}}


`@script`
Files can be imported with incorrect data types, making your data harder to analyze. Datasets often have missing values, custom missing value designators, or records that have errors. Luckily, the pandas read_csv method offers ways to address these issues at the import stage, reducing the wrangling you have to do later on.


---
## Specifying Data Types

```yaml
type: "TwoRows"
key: "22caa3e64d"
```

`@part1`
![](https://lh6.googleusercontent.com/Jx-Tu0YVqi4Mfo5X3DSMCTm0L76o7pr3dcpagj71-8h7jck8w-XamQX0xx7EYMWqqV0EDIcBVLfOkQ=w1920-h903) {{1}}


`@part2`
```
import pandas as pd

data = pd.read_csv(file, dtype={‘zipcode’: str})
```{{2}}


`@script`
When you read flat files with pandas, pandas infers each column's data type. However, it doesn’t always get it right. For example, here is a csv of 2016 tax return data by ZIP code for the state of Vermont. By default, pandas will interpret zipcode as an integer field, but we want them to be strings.
Instead of letting pandas guess, we can tell it exactly what data type any or all of the fields should be using the ```dtype``` parameter in ```read_csv```.
```dtype``` takes a dictionary, where each key is a field and each value is the data type that field should be.


---
## Setting Custom Missing Data Values

```yaml
type: "FullSlide"
key: "c4ac8549d0"
```

`@part1`
![](https://lh6.googleusercontent.com/CC8of9zaUZp1bAdckaWfo_yl5zmWRX76OdXwCjKXqoo0BQ2TwA34fhYuE6tyOvAgLB69qz8Ddj36aQ=w1920-h903) {{1}}

```
import pandas as pd
file = 'vermont_tax_data_2016.csv'
data = pd.read_csv(file, dtype={'zipcode': str}, na_values='0')
```{{2}}

![](https://lh4.googleusercontent.com/tnrQHAZ39fV9MdARLzMwFycONSiZ-EVCBZvsovrtyRAqylBmyBa5c-xWKpqs2vTRo2qcaCcNZOQC1Q=w1920-h903){{3}}


`@script`
Pandas automatically looks out for certain values, like “N/A” or “null”,  when importing data and re-codes them as missing values, allowing you to make use of some handy data-cleaning functions later on. But missing values in your data might be represented differently, such as with a dummy code or the text “none”. Back in our Vermont tax data, some records had a ZIP code value of 0, which is not a valid postal code and should be treated as missing.
You can tell pandas to treat these entries as missing values with the read_csv's na_values parameter, which can take a value or list of values that should be coded as missing. We'll record the 0 zip codes as missing. You can even set column-specific N/A values by passing in a dictionary, where each key-value pair represents a field and a list of values in that field to treat as N/As.


---
## Lines with Errors

```yaml
type: "TwoRows"
key: "e03fcbdf7b"
```

`@part1`
![](https://lh5.googleusercontent.com/dbx6ZHFREHw7HtVvKtDkDfQhpVE5h5j0GWQIoK4_v3u7H1JYyCCpmEW6s0-7Evvv6lO_Of78sunbSg=w1920-h903) {{1}}


`@part2`
```
import pandas as pd
data = pd.read_csv(file, error_bad_lines=False, warn_bad_lines=True)
``` {{2}}


`@script`
One last issue you may face are lines that pandas just can't parse. For example, a record could have more values than there are columns, like the second record in this corrupted version of the Vermont tax data.

In this case, you will get an error that pandas could not parse the file. Luckily, there is an option to skip unparseable lines and import the rest of the data. Setting the error_bad_lines parameter to False will cause pandas to skip bad records but continue the import. It's a good idea to set the warn_bad_lines parameter to True so you can see if any lines are being skipped.

A word of caution: if you find that lines are being skipped due to errors, it's a good idea to investigate what's being left out and why, to see if there are underlying issues that should be addressed or accounted for in your analysis.


---
## Practice!

```yaml
type: "FinalSlide"
key: "bc2d10733a"
```

`@script`
In this video, you've learned about some common issues analysts face when importing data from flat files, and how to handle those issues. Now, it's your turn to practice. Good luck!

